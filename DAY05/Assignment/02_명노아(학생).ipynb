{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import koreanize_matplotlib\n",
    "import seaborn as sns\n",
    "\n",
    "df=pd.read_csv(\"student.csv\", sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 82., 104.,  98.,   0.,  82.,  24.,   0.,   3.,   1.,   1.]),\n",
       " array([15. , 15.7, 16.4, 17.1, 17.8, 18.5, 19.2, 19.9, 20.6, 21.3, 22. ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(df[\"age\"]) # 꿇었구나.... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.info() # target이 성적이므로, 모든 요소 하나하나가 다 영향이 있다..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모든 데이터를 정수로 변환\n",
    "from sklearn.preprocessing import LabelEncoder \n",
    "le=LabelEncoder()\n",
    "for i in df.columns:\n",
    "    if df[i].dtype == \"object\": # 컬럼의 타입이 \"Object\"이면,\n",
    "        df[i] = le.fit_transform(df[i]) # LabelEncoder로 문자열을 숫자로 변환\n",
    "#df.info() # 모두 숫자로 변환되었음 => 다만, 0으로 낮게 나오는 경우가 있으니, 이를 특정 값으로 대체하여야만 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#별 짓을 해도 안되니, 내 생각대로 내 컬럼을 만들꺼다...\n",
    "# => Medu * Fedu : 값이 높을수록 좋다 \n",
    "# => reason * famrel : 학교를 선택한 이유가 학교 평판과 과정이면 높은 점수, 가족관계가 좋으면 높은 점수\n",
    "# => Dalc * Walc : 음주가 적을수록 좋다\n",
    "# => goout * freetime : 외출과 자유시간이 많을수록 좋다\n",
    "\n",
    "df[\"Parent_level\"]=df[\"Medu\"]*df[\"Fedu\"]\n",
    "df[\"Reputation\"]=df[\"reason\"]*df[\"famrel\"]^2 \n",
    "df[\"Bear\"]=df[\"Dalc\"]*df[\"Walc\"]\n",
    "df[\"freedom\"]=df[\"goout\"]*df[\"freetime\"]\n",
    "\n",
    "# => 잠깐...LabelEncoder말고 내가 직접 하자...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>school</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>address</th>\n",
       "      <th>famsize</th>\n",
       "      <th>Pstatus</th>\n",
       "      <th>Medu</th>\n",
       "      <th>Fedu</th>\n",
       "      <th>Mjob</th>\n",
       "      <th>Fjob</th>\n",
       "      <th>...</th>\n",
       "      <th>Walc</th>\n",
       "      <th>health</th>\n",
       "      <th>absences</th>\n",
       "      <th>G1</th>\n",
       "      <th>G2</th>\n",
       "      <th>G3</th>\n",
       "      <th>Parent_level</th>\n",
       "      <th>Reputation</th>\n",
       "      <th>Bear</th>\n",
       "      <th>freedom</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>14</td>\n",
       "      <td>15</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>18</td>\n",
       "      <td>19</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   school  sex  age  address  famsize  Pstatus  Medu  Fedu  Mjob  Fjob  ...  \\\n",
       "0       0    0   18        1        0        0     4     4     0     4  ...   \n",
       "1       0    0   17        1        0        1     1     1     0     2  ...   \n",
       "2       0    0   15        1        1        1     1     1     0     2  ...   \n",
       "3       0    0   15        1        0        1     4     2     1     3  ...   \n",
       "4       0    0   16        1        0        1     3     3     2     2  ...   \n",
       "5       0    1   16        1        1        1     4     3     3     2  ...   \n",
       "6       0    1   16        1        1        1     2     2     2     2  ...   \n",
       "7       0    0   17        1        0        0     4     4     2     4  ...   \n",
       "8       0    1   15        1        1        0     3     2     3     2  ...   \n",
       "9       0    1   15        1        0        1     3     4     2     2  ...   \n",
       "\n",
       "   Walc  health  absences  G1  G2  G3  Parent_level  Reputation  Bear  freedom  \n",
       "0     1       3         6   5   6   6            16           2     1       12  \n",
       "1     1       3         4   5   5   6             1           2     1        9  \n",
       "2     3       3        10   7   8  10             1          10     6        6  \n",
       "3     1       5         2  15  14  15             8           1     1        4  \n",
       "4     2       5         4   6  10  10             9           6     2        6  \n",
       "5     2       5        10  15  15  15            12          13     2        8  \n",
       "6     1       3         0  12  12  11             4           6     1       16  \n",
       "7     1       1         6   6   5   6            16           6     1        4  \n",
       "8     1       1         0  16  18  19             6           6     1        4  \n",
       "9     1       5         0  14  15  15            12           7     1        5  \n",
       "\n",
       "[10 rows x 37 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df.columns: # 0,1의 이진 값을 +-값을 주어 극대화시킴 \n",
    "    if len(df[i].unique())==2:\n",
    "        df[i].replace({0:-5, 1:5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Variable</th>\n",
       "      <th>VIF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>age</td>\n",
       "      <td>109.602726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Medu</td>\n",
       "      <td>56.861890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Fedu</td>\n",
       "      <td>66.296051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>G1</td>\n",
       "      <td>52.225719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>G2</td>\n",
       "      <td>74.527531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Parent_level</td>\n",
       "      <td>77.019677</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Variable         VIF\n",
       "2            age  109.602726\n",
       "6           Medu   56.861890\n",
       "7           Fedu   66.296051\n",
       "30            G1   52.225719\n",
       "31            G2   74.527531\n",
       "33  Parent_level   77.019677"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 다중공산성(multicollinearity)을 없애기 위해, 상관계수가 강한 컬럼 제거\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "\n",
    "# 독립 변수들의 데이터프레임 넣기\n",
    "X = df[df.columns[:-3]]\n",
    "\n",
    "# VIF 계산\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"Variable\"] = X.columns\n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "\n",
    "# VIF 출력 : 10이 넘는 변수들은 다중공산성이 심한 독립변수들 => 제거하여 마땅하도다.... \n",
    "VIF_result=vif_data[vif_data[\"VIF\"] > 50]\n",
    "VIF_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 해당 컬럼을 제거 => 다중공산성 문제 해결! \n",
    "df.drop(VIF_result[\"Variable\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 중복 데이터 확인 => 한개도 없다 : 그냥 써도 좋은 아주 훌륭한 데이터! \n",
    "#df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이상값 제거\n",
    "import scipy.stats as stats \n",
    "z = np.abs(stats.zscore(df)) \n",
    "index,_=np.where(z>3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이상치를 심하게 웃도는 데이터 제거 \n",
    "df.drop(index, axis=0, inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['school', 'sex', 'address', 'famsize', 'Pstatus', 'Mjob', 'Fjob',\n",
       "       'reason', 'guardian', 'traveltime', 'studytime', 'failures',\n",
       "       'schoolsup', 'famsup', 'paid', 'activities', 'nursery', 'higher',\n",
       "       'internet', 'romantic', 'famrel', 'freetime', 'goout', 'Dalc', 'Walc',\n",
       "       'health', 'absences', 'G3', 'Reputation', 'Bear', 'freedom'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'school+sex+address+famsize+Pstatus+Mjob+Fjob+reason+guardian+traveltime+studytime+failures+schoolsup+famsup+paid+activities+nursery+higher+internet+romantic+famrel+freetime+goout+Dalc+Walc+health+absences'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"+\".join(df.columns[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>           <td>G3</td>        <th>  R-squared:         </th> <td>   0.220</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.156</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   3.394</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Mon, 04 Mar 2024</td> <th>  Prob (F-statistic):</th> <td>1.68e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>09:24:51</td>     <th>  Log-Likelihood:    </th> <td> -946.33</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   339</td>      <th>  AIC:               </th> <td>   1947.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   312</td>      <th>  BIC:               </th> <td>   2050.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    26</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "       <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>  <td>    4.7946</td> <td>    1.153</td> <td>    4.159</td> <td> 0.000</td> <td>    2.526</td> <td>    7.063</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>school</th>     <td>    0.3403</td> <td>    0.825</td> <td>    0.413</td> <td> 0.680</td> <td>   -1.282</td> <td>    1.963</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>sex</th>        <td>    1.4133</td> <td>    0.521</td> <td>    2.714</td> <td> 0.007</td> <td>    0.389</td> <td>    2.438</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>address</th>    <td>    0.7238</td> <td>    0.621</td> <td>    1.166</td> <td> 0.245</td> <td>   -0.498</td> <td>    1.945</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>famsize</th>    <td>    0.4899</td> <td>    0.519</td> <td>    0.944</td> <td> 0.346</td> <td>   -0.531</td> <td>    1.511</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Pstatus</th>    <td>   -0.6634</td> <td>    0.783</td> <td>   -0.848</td> <td> 0.397</td> <td>   -2.203</td> <td>    0.877</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Mjob</th>       <td>   -0.1152</td> <td>    0.207</td> <td>   -0.556</td> <td> 0.579</td> <td>   -0.523</td> <td>    0.293</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Fjob</th>       <td>    0.4530</td> <td>    0.268</td> <td>    1.691</td> <td> 0.092</td> <td>   -0.074</td> <td>    0.980</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>reason</th>     <td>    0.4604</td> <td>    0.191</td> <td>    2.409</td> <td> 0.017</td> <td>    0.084</td> <td>    0.837</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>guardian</th>   <td>   -0.0635</td> <td>    0.446</td> <td>   -0.143</td> <td> 0.887</td> <td>   -0.941</td> <td>    0.813</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>traveltime</th> <td>   -0.7308</td> <td>    0.414</td> <td>   -1.766</td> <td> 0.078</td> <td>   -1.545</td> <td>    0.084</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>studytime</th>  <td>    0.3836</td> <td>    0.297</td> <td>    1.291</td> <td> 0.198</td> <td>   -0.201</td> <td>    0.968</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>failures</th>   <td>   -2.4213</td> <td>    0.497</td> <td>   -4.874</td> <td> 0.000</td> <td>   -3.399</td> <td>   -1.444</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>schoolsup</th>  <td>   -1.2385</td> <td>    0.689</td> <td>   -1.798</td> <td> 0.073</td> <td>   -2.594</td> <td>    0.117</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>famsup</th>     <td>   -0.7202</td> <td>    0.505</td> <td>   -1.425</td> <td> 0.155</td> <td>   -1.715</td> <td>    0.274</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>paid</th>       <td>    0.1661</td> <td>    0.503</td> <td>    0.330</td> <td> 0.742</td> <td>   -0.824</td> <td>    1.157</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>activities</th> <td>   -0.4782</td> <td>    0.479</td> <td>   -0.999</td> <td> 0.318</td> <td>   -1.420</td> <td>    0.463</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>nursery</th>    <td>    0.4691</td> <td>    0.589</td> <td>    0.797</td> <td> 0.426</td> <td>   -0.690</td> <td>    1.628</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>higher</th>     <td>    4.7946</td> <td>    1.153</td> <td>    4.159</td> <td> 0.000</td> <td>    2.526</td> <td>    7.063</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>internet</th>   <td>    0.9897</td> <td>    0.655</td> <td>    1.512</td> <td> 0.132</td> <td>   -0.298</td> <td>    2.278</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>romantic</th>   <td>   -0.7971</td> <td>    0.519</td> <td>   -1.536</td> <td> 0.125</td> <td>   -1.818</td> <td>    0.224</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>famrel</th>     <td>    0.1170</td> <td>    0.297</td> <td>    0.394</td> <td> 0.694</td> <td>   -0.468</td> <td>    0.702</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>freetime</th>   <td>    0.2284</td> <td>    0.250</td> <td>    0.915</td> <td> 0.361</td> <td>   -0.263</td> <td>    0.720</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>goout</th>      <td>   -0.5229</td> <td>    0.245</td> <td>   -2.135</td> <td> 0.034</td> <td>   -1.005</td> <td>   -0.041</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Dalc</th>       <td>   -0.1575</td> <td>    0.428</td> <td>   -0.368</td> <td> 0.713</td> <td>   -1.000</td> <td>    0.685</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Walc</th>       <td>   -0.0825</td> <td>    0.264</td> <td>   -0.313</td> <td> 0.755</td> <td>   -0.601</td> <td>    0.436</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>health</th>     <td>    0.0008</td> <td>    0.171</td> <td>    0.005</td> <td> 0.996</td> <td>   -0.336</td> <td>    0.337</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>absences</th>   <td>    0.0915</td> <td>    0.043</td> <td>    2.113</td> <td> 0.035</td> <td>    0.006</td> <td>    0.177</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>21.745</td> <th>  Durbin-Watson:     </th> <td>   2.093</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td> <th>  Jarque-Bera (JB):  </th> <td>  24.119</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.612</td> <th>  Prob(JB):          </th> <td>5.79e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 3.457</td> <th>  Cond. No.          </th> <td>3.92e+16</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The smallest eigenvalue is 2.52e-29. This might indicate that there are<br/>strong multicollinearity problems or that the design matrix is singular."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}    &        G3        & \\textbf{  R-squared:         } &     0.220   \\\\\n",
       "\\textbf{Model:}            &       OLS        & \\textbf{  Adj. R-squared:    } &     0.156   \\\\\n",
       "\\textbf{Method:}           &  Least Squares   & \\textbf{  F-statistic:       } &     3.394   \\\\\n",
       "\\textbf{Date:}             & Mon, 04 Mar 2024 & \\textbf{  Prob (F-statistic):} &  1.68e-07   \\\\\n",
       "\\textbf{Time:}             &     09:24:51     & \\textbf{  Log-Likelihood:    } &   -946.33   \\\\\n",
       "\\textbf{No. Observations:} &         339      & \\textbf{  AIC:               } &     1947.   \\\\\n",
       "\\textbf{Df Residuals:}     &         312      & \\textbf{  BIC:               } &     2050.   \\\\\n",
       "\\textbf{Df Model:}         &          26      & \\textbf{                     } &             \\\\\n",
       "\\textbf{Covariance Type:}  &    nonrobust     & \\textbf{                     } &             \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                    & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{Intercept}  &       4.7946  &        1.153     &     4.159  &         0.000        &        2.526    &        7.063     \\\\\n",
       "\\textbf{school}     &       0.3403  &        0.825     &     0.413  &         0.680        &       -1.282    &        1.963     \\\\\n",
       "\\textbf{sex}        &       1.4133  &        0.521     &     2.714  &         0.007        &        0.389    &        2.438     \\\\\n",
       "\\textbf{address}    &       0.7238  &        0.621     &     1.166  &         0.245        &       -0.498    &        1.945     \\\\\n",
       "\\textbf{famsize}    &       0.4899  &        0.519     &     0.944  &         0.346        &       -0.531    &        1.511     \\\\\n",
       "\\textbf{Pstatus}    &      -0.6634  &        0.783     &    -0.848  &         0.397        &       -2.203    &        0.877     \\\\\n",
       "\\textbf{Mjob}       &      -0.1152  &        0.207     &    -0.556  &         0.579        &       -0.523    &        0.293     \\\\\n",
       "\\textbf{Fjob}       &       0.4530  &        0.268     &     1.691  &         0.092        &       -0.074    &        0.980     \\\\\n",
       "\\textbf{reason}     &       0.4604  &        0.191     &     2.409  &         0.017        &        0.084    &        0.837     \\\\\n",
       "\\textbf{guardian}   &      -0.0635  &        0.446     &    -0.143  &         0.887        &       -0.941    &        0.813     \\\\\n",
       "\\textbf{traveltime} &      -0.7308  &        0.414     &    -1.766  &         0.078        &       -1.545    &        0.084     \\\\\n",
       "\\textbf{studytime}  &       0.3836  &        0.297     &     1.291  &         0.198        &       -0.201    &        0.968     \\\\\n",
       "\\textbf{failures}   &      -2.4213  &        0.497     &    -4.874  &         0.000        &       -3.399    &       -1.444     \\\\\n",
       "\\textbf{schoolsup}  &      -1.2385  &        0.689     &    -1.798  &         0.073        &       -2.594    &        0.117     \\\\\n",
       "\\textbf{famsup}     &      -0.7202  &        0.505     &    -1.425  &         0.155        &       -1.715    &        0.274     \\\\\n",
       "\\textbf{paid}       &       0.1661  &        0.503     &     0.330  &         0.742        &       -0.824    &        1.157     \\\\\n",
       "\\textbf{activities} &      -0.4782  &        0.479     &    -0.999  &         0.318        &       -1.420    &        0.463     \\\\\n",
       "\\textbf{nursery}    &       0.4691  &        0.589     &     0.797  &         0.426        &       -0.690    &        1.628     \\\\\n",
       "\\textbf{higher}     &       4.7946  &        1.153     &     4.159  &         0.000        &        2.526    &        7.063     \\\\\n",
       "\\textbf{internet}   &       0.9897  &        0.655     &     1.512  &         0.132        &       -0.298    &        2.278     \\\\\n",
       "\\textbf{romantic}   &      -0.7971  &        0.519     &    -1.536  &         0.125        &       -1.818    &        0.224     \\\\\n",
       "\\textbf{famrel}     &       0.1170  &        0.297     &     0.394  &         0.694        &       -0.468    &        0.702     \\\\\n",
       "\\textbf{freetime}   &       0.2284  &        0.250     &     0.915  &         0.361        &       -0.263    &        0.720     \\\\\n",
       "\\textbf{goout}      &      -0.5229  &        0.245     &    -2.135  &         0.034        &       -1.005    &       -0.041     \\\\\n",
       "\\textbf{Dalc}       &      -0.1575  &        0.428     &    -0.368  &         0.713        &       -1.000    &        0.685     \\\\\n",
       "\\textbf{Walc}       &      -0.0825  &        0.264     &    -0.313  &         0.755        &       -0.601    &        0.436     \\\\\n",
       "\\textbf{health}     &       0.0008  &        0.171     &     0.005  &         0.996        &       -0.336    &        0.337     \\\\\n",
       "\\textbf{absences}   &       0.0915  &        0.043     &     2.113  &         0.035        &        0.006    &        0.177     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       & 21.745 & \\textbf{  Durbin-Watson:     } &    2.093  \\\\\n",
       "\\textbf{Prob(Omnibus):} &  0.000 & \\textbf{  Jarque-Bera (JB):  } &   24.119  \\\\\n",
       "\\textbf{Skew:}          & -0.612 & \\textbf{  Prob(JB):          } & 5.79e-06  \\\\\n",
       "\\textbf{Kurtosis:}      &  3.457 & \\textbf{  Cond. No.          } & 3.92e+16  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. \\newline\n",
       " [2] The smallest eigenvalue is 2.52e-29. This might indicate that there are \\newline\n",
       " strong multicollinearity problems or that the design matrix is singular."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                     G3   R-squared:                       0.220\n",
       "Model:                            OLS   Adj. R-squared:                  0.156\n",
       "Method:                 Least Squares   F-statistic:                     3.394\n",
       "Date:                Mon, 04 Mar 2024   Prob (F-statistic):           1.68e-07\n",
       "Time:                        09:24:51   Log-Likelihood:                -946.33\n",
       "No. Observations:                 339   AIC:                             1947.\n",
       "Df Residuals:                     312   BIC:                             2050.\n",
       "Df Model:                          26                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "Intercept      4.7946      1.153      4.159      0.000       2.526       7.063\n",
       "school         0.3403      0.825      0.413      0.680      -1.282       1.963\n",
       "sex            1.4133      0.521      2.714      0.007       0.389       2.438\n",
       "address        0.7238      0.621      1.166      0.245      -0.498       1.945\n",
       "famsize        0.4899      0.519      0.944      0.346      -0.531       1.511\n",
       "Pstatus       -0.6634      0.783     -0.848      0.397      -2.203       0.877\n",
       "Mjob          -0.1152      0.207     -0.556      0.579      -0.523       0.293\n",
       "Fjob           0.4530      0.268      1.691      0.092      -0.074       0.980\n",
       "reason         0.4604      0.191      2.409      0.017       0.084       0.837\n",
       "guardian      -0.0635      0.446     -0.143      0.887      -0.941       0.813\n",
       "traveltime    -0.7308      0.414     -1.766      0.078      -1.545       0.084\n",
       "studytime      0.3836      0.297      1.291      0.198      -0.201       0.968\n",
       "failures      -2.4213      0.497     -4.874      0.000      -3.399      -1.444\n",
       "schoolsup     -1.2385      0.689     -1.798      0.073      -2.594       0.117\n",
       "famsup        -0.7202      0.505     -1.425      0.155      -1.715       0.274\n",
       "paid           0.1661      0.503      0.330      0.742      -0.824       1.157\n",
       "activities    -0.4782      0.479     -0.999      0.318      -1.420       0.463\n",
       "nursery        0.4691      0.589      0.797      0.426      -0.690       1.628\n",
       "higher         4.7946      1.153      4.159      0.000       2.526       7.063\n",
       "internet       0.9897      0.655      1.512      0.132      -0.298       2.278\n",
       "romantic      -0.7971      0.519     -1.536      0.125      -1.818       0.224\n",
       "famrel         0.1170      0.297      0.394      0.694      -0.468       0.702\n",
       "freetime       0.2284      0.250      0.915      0.361      -0.263       0.720\n",
       "goout         -0.5229      0.245     -2.135      0.034      -1.005      -0.041\n",
       "Dalc          -0.1575      0.428     -0.368      0.713      -1.000       0.685\n",
       "Walc          -0.0825      0.264     -0.313      0.755      -0.601       0.436\n",
       "health         0.0008      0.171      0.005      0.996      -0.336       0.337\n",
       "absences       0.0915      0.043      2.113      0.035       0.006       0.177\n",
       "==============================================================================\n",
       "Omnibus:                       21.745   Durbin-Watson:                   2.093\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               24.119\n",
       "Skew:                          -0.612   Prob(JB):                     5.79e-06\n",
       "Kurtosis:                       3.457   Cond. No.                     3.92e+16\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The smallest eigenvalue is 2.52e-29. This might indicate that there are\n",
       "strong multicollinearity problems or that the design matrix is singular.\n",
       "\"\"\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ols 분석 시행하여 유의미한 변수를 찾음\n",
    "import statsmodels.formula.api as smf \n",
    "formula = f'G3 ~ {\"+\".join(df.columns[:-4])}'\n",
    "result = smf.ols(formula, df).fit()\n",
    "result.summary()\n",
    "# Adj.R-squared : 0.214 -> 최악이다....\n",
    "# Log-Likelihood(우도)가 낮고, AIC, BIC 값이 크다 : 예측이 힘들다\n",
    "# Skew : 데이터는 정규분포에 가까움 \n",
    "# Kurtosis : 3과 근사하므로, 정규분포와 유사한 형태를 띄고 있음\n",
    "# Cond. No. : 다중공산성은 낮은 편 -> 이건 좋다! \n",
    "\n",
    "# ==> 즉, P-value가 0.1(유의수준)보다 낮고, 높은 선형성의 관계를 갖는 값\n",
    "# reason, traveltime, failures, schoolsup, absences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "activities    0.022752\n",
       "health        0.023952\n",
       "freetime      0.031608\n",
       "paid          0.035131\n",
       "absences      0.035728\n",
       "studytime     0.041778\n",
       "freedom       0.050468\n",
       "Bear          0.052562\n",
       "Walc          0.053480\n",
       "Pstatus       0.055857\n",
       "school        0.056143\n",
       "guardian      0.060695\n",
       "Dalc          0.063292\n",
       "famsize       0.070622\n",
       "famrel        0.070942\n",
       "famsup        0.075115\n",
       "Fjob          0.075560\n",
       "nursery       0.079252\n",
       "Mjob          0.087757\n",
       "reason        0.107940\n",
       "goout         0.108168\n",
       "internet      0.109191\n",
       "address       0.119184\n",
       "schoolsup     0.119656\n",
       "romantic      0.121598\n",
       "Reputation    0.128341\n",
       "traveltime    0.143283\n",
       "sex           0.163893\n",
       "failures      0.309576\n",
       "G3            1.000000\n",
       "higher             NaN\n",
       "Name: G3, dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.abs(df.corr()[\"G3\"]).sort_values() # 상관계수 값이 낮으므로, 그나마 높을 걸 뽑는다\n",
    "# ols => reason, traveltime, failures, schoolsup, absences      \n",
    "# corr => sex, traveltime, failures    \n",
    "# 최종적 : traveltime, failures, + sex, reason으로 feature 결정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최종적 : traveltime, failures, + sex, reason으로 feature 결정\n",
    "from sklearn.model_selection import train_test_split \n",
    "feature=df[[\"traveltime\",\"failures\",\"Reputation\"]]\n",
    "target=df[\"G3\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures # 열 추가 방법 시도....1\n",
    "\n",
    "\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False) # PolyFeatures 객체 생성\n",
    "df2=pd.DataFrame(poly.fit_transform(feature), columns=poly.get_feature_names_out()) # DataFrame으로 변환\n",
    "df = pd.concat([target,df2],axis=1) # DataFrame 병합 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import FunctionTransformer # 열 추가 방법 시도....2\n",
    "\n",
    "# def custom_function(X):\n",
    "#     return np.column_stack((X, np.sin(X), np.cos(X)))\n",
    "\n",
    "# poly = FunctionTransformer(func=custom_function)\n",
    "# df2=pd.DataFrame(poly.fit_transform(feature)) # DataFrame으로 변환\n",
    "# df = pd.concat([target,df2],axis=1) # DataFrame 병합 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(feature, \n",
    "                                                    target, \n",
    "                                                    test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\EXAM_ML\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:2667: UserWarning: n_quantiles (100) is greater than the total number of samples (68). n_quantiles is set to n_samples.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler     \n",
    "from sklearn.preprocessing import MinMaxScaler     \n",
    "from sklearn.preprocessing import RobustScaler     \n",
    "from sklearn.preprocessing import SplineTransformer     \n",
    "from sklearn.preprocessing import QuantileTransformer   \n",
    "\n",
    "std_s=StandardScaler()\n",
    "x_train_std=std_s.fit_transform(x_train)\n",
    "std_s=StandardScaler()\n",
    "x_test_std=std_s.fit_transform(x_test)\n",
    "\n",
    "mm_s=MinMaxScaler()\n",
    "x_train_mm=mm_s.fit_transform(x_train)\n",
    "mm_s=MinMaxScaler()\n",
    "x_test_mm=mm_s.fit_transform(x_test)\n",
    "\n",
    "ro_s=RobustScaler()\n",
    "x_train_ro=ro_s.fit_transform(x_train)\n",
    "ro_s=RobustScaler()\n",
    "x_test_ro=ro_s.fit_transform(x_test)\n",
    "\n",
    "sq_s=SplineTransformer()\n",
    "x_train_sq=sq_s.fit_transform(x_train)\n",
    "sq_s=SplineTransformer()\n",
    "x_test_sq=sq_s.fit_transform(x_test)\n",
    "\n",
    "qt_s=QuantileTransformer(n_quantiles=100)\n",
    "x_train_qt=qt_s.fit_transform(x_train)\n",
    "qt_s=QuantileTransformer(n_quantiles=100)\n",
    "x_test_qt=qt_s.fit_transform(x_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data=[x_train_std,x_test_std,x_train_mm,x_test_mm,x_train_ro,x_test_ro,x_train_sq,x_test_sq,x_train_qt,x_test_qt]\n",
    "len(x_data) # 2X5=10개의 X 데이터 셋 마련 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier         \n",
    "from sklearn.ensemble import RandomForestRegressor     \n",
    "from sklearn.svm import SVR     \n",
    "from sklearn.linear_model import LogisticRegression     \n",
    "from sklearn.neighbors import KNeighborsRegressor     \n",
    "from sklearn.linear_model import LinearRegression        \n",
    "from sklearn.multiclass import OneVsRestClassifier     \n",
    "from sklearn.multiclass import OneVsOneClassifier        \n",
    "from sklearn.metrics import mean_squared_error     \n",
    "from sklearn.metrics import mean_absolute_error     \n",
    "from sklearn.metrics import r2_score       \n",
    "from sklearn.metrics import accuracy_score     \n",
    "from sklearn.metrics import precision_score     \n",
    "from sklearn.metrics import recall_score     \n",
    "from sklearn.metrics import f1_score     \n",
    "from sklearn.metrics import confusion_matrix     \n",
    "from sklearn.metrics import classification_report \n",
    "\n",
    "model_list=[KNeighborsClassifier(), \n",
    "            LogisticRegression(), # 여기까지는 분류 측정 지표 계산 가능\n",
    "            RandomForestRegressor(),\n",
    "            SVR(),\n",
    "            KNeighborsRegressor(),\n",
    "            LinearRegression()] \n",
    "#OvO_list=[OneVsRestClassifier(),OneVsOneClassifier()]\n",
    "\n",
    "score_dict=dict()\n",
    "for j in range(len(model_list)):\n",
    "    for i in range(0,10,2) :\n",
    "        model=model_list[j]\n",
    "        model.fit(x_data[i], y_train)\n",
    "        train_score=model.score(x_data[i], y_train)                # (1) train_score  \n",
    "        test_score=model.score(x_data[i+1], y_test)                # (2) test_score  \n",
    "        \n",
    "        y_pre=model.predict(x_data[i+1])\n",
    "        rmse=mean_squared_error(y_test, y_pre, squared=False)      # (3) RMSE  \n",
    "        mse=mean_squared_error(y_test, y_pre, squared=True)        # (4) MSE  \n",
    "        r2=r2_score(y_test, y_pre)                                 # (5) R^2  \n",
    "        mae=mean_absolute_error(y_test, y_pre)                     # (6) mae \n",
    "        \n",
    "        if j in [0,1] :\n",
    "            cr=classification_report(y_test, y_pre, zero_division=1)   # (7) classification_report\n",
    "            f1=f1_score(y_test, y_pre, average='weighted', zero_division=1)             # (8) f1 \n",
    "            recall=recall_score(y_test, y_pre, average='micro', zero_division=1)        # (9) recall\n",
    "            cm=confusion_matrix(y_test, y_pre)                         # (10) confusion_matrix\n",
    "            accuracy=accuracy_score(y_test, y_pre)                     # (11) accuracy\n",
    "            precision=precision_score(y_test, y_pre, average='macro', zero_division=1)  # (12) precision\n",
    "\n",
    "            score_dict.setdefault(j,[train_score, test_score, rmse,mse,r2,mae,cr,f1,recall,cm,accuracy,precision])\n",
    "        else :\n",
    "            score_dict.setdefault(j,[train_score, test_score, rmse,mse,np.nan,np.nan,np.nan,np.nan,np.nan,np.nan,np.nan,np.nan])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier     0.102941\n",
       "LogisticRegression       0.132353\n",
       "RandomForestRegressor   -0.153725\n",
       "SVR                      0.012859\n",
       "KNeighborsRegressor     -0.225404\n",
       "LinearRegression         0.057802\n",
       "Name: test_score, dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_df=pd.DataFrame(score_dict, index=['train_score','test_score','RMSE','MSE','R^2','MAE','classification_report','f1','recall','confusion_matrix','accuracy','precision'])\n",
    "score_df.columns=[\"KNeighborsClassifier\",\"LogisticRegression\",\"RandomForestRegressor\",\"SVR\",\"KNeighborsRegressor\",\"LinearRegression\"]\n",
    "score_df.loc[\"test_score\"] # 13....?\n",
    "\n",
    "# 일단 KNeiborsClassifier과, LogisticRegression이 높게 나왔다\n",
    "# 해당 코드 파일을 통해서, 튜닝할 모델을 골랐으니, 다시 정확도를 높여보자! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#별 짓을 해도 안되니, 내 생각대로 내 컬럼을 만들꺼다...\n",
    "# 4. address 주소 -> U(도시), R(농촌)\n",
    "# 5. famsize 가족 크기\n",
    "# 6. Pstatus 부모의 동거 여부 ->  T:동거 A:동거 X\n",
    "\n",
    "\n",
    "# 7. Medu 어머니의 교육 -> 0:없음 1:초등교육(4학년) 2:5-9학년 3:중등교육 4:고등교육\n",
    "# 8. Fedu 아버지의 교육 -> 0:없음 1:초등교육(4학년) 2:5-9학년 3:중등교육 4:고등교육\n",
    "#=> Medu + Fedu 를 합치자\n",
    "\n",
    "# 9. Mjob 어머니의 직업 -> nominal(교사,건강관리 관련), services(행정,경찰)\n",
    "# 10. Fjob 아버지의 직업 -> nominal(교사,건강관리 관련), services(행정,경찰)\n",
    "\n",
    "# 11. reason 본 학교를 선택한 이유 -> 집에 가까운, 학교 평판, 과정, 선호, 기타\n",
    "# 12. guardian 학생의 보호자 -> mother, father, 기타\n",
    "\n",
    "# 13. traveltime 이동 시간 -> 15분 이상 -> yes , no\n",
    "\n",
    "# 14. studytime 학습 시간 -> 1:2시간이하 2:2-5시간 3:5-10시간 4:10시간이상\n",
    "# 15. failures 낙제 횟수 -> 1,2,3 or 4(1,2,3 아니면 4)\n",
    "# 16. schoolsup 추가교육(보충수업?)\n",
    "# 17. famsup 가족 교육 지원\n",
    "# 18. paid 수학 공부 추가 지원 여부\n",
    "# 19. activities 과외 활동\n",
    "# 20. nursery 보육원\n",
    "# 21. higher 고등교육 희망 유무\n",
    "# 22. internet 인터넷 접속\n",
    "# 23. romantic 이성교제\n",
    "# 24. famrel 가족관계 -> 1:매우나쁨 2:나쁨 3:보통 4:좋음 5:매우좋음\n",
    "# 25. freetime 자유시간 -> 1:매우적음 2:적음 3:보통 4:많음 5:매우많음\n",
    "# 26. goout 외출 -> 1:매우적음 2:적음 3:보통 4:많음 5:매우많음\n",
    "# 27. Dalc 일과 중 음주 -> 1:매우적음 2:적음 3:보통 4:많음 5:매우많음\n",
    "# 28. Walc 주말 음주 -> 1:매우적음 2:적음 3:보통 4:많음 5:매우많음\n",
    "# 29. health 현재 건강 상태 -> 1:매우나쁨 2:나쁨 3:보통 4:좋음 5:매우좋음\n",
    "\n",
    "\n",
    "# => reason + famrel\n",
    "# => Dalc + Walc \n",
    "# => goout + freetime"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EXAM_ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
