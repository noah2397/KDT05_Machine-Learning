{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import koreanize_matplotlib\n",
    "import seaborn as sns\n",
    "\n",
    "df=pd.read_csv(\"student.csv\", sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 38.,   0.,   8.,  24.,  60., 103.,  62.,  60.,  22.,  18.]),\n",
       " array([ 0.,  2.,  4.,  6.,  8., 10., 12., 14., 16., 18., 20.]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(df[\"G3\"]) # 이런...양아치들이 있었다\n",
    "# 0점인 학생과 아닌 학생을 분리하면, 정확도가 더 높아질 것 같다! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_0=df[df[\"G3\"]==0] # 성적이 0점인 학생들은 따로 분리\n",
    "df=df[df[\"G3\"]!=0] # 기존의 df변수는 0점인 학생들을 제외하고 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문자 형식의 열은 삭제\n",
    "for i in df.columns:\n",
    "    if df[i].dtype == \"object\": # 컬럼의 타입이 \"Object\"이면,\n",
    "        df.drop(i, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이상값 제거\n",
    "import scipy.stats as stats \n",
    "z = np.abs(stats.zscore(df)) \n",
    "index,_=np.where(z>2.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이상치를 심하게 웃도는 데이터 제거 \n",
    "df.drop(index, axis=0, inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['age', 'Medu', 'Fedu', 'traveltime', 'studytime', 'failures', 'famrel',\n",
       "       'freetime', 'goout', 'Dalc', 'Walc', 'health', 'absences', 'G1', 'G2',\n",
       "       'G3'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'age+Medu+Fedu+traveltime+studytime+failures+famrel+freetime+goout+Dalc+Walc+health+absences'"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "med=df[df.columns[:13]]\n",
    "strr=\"+\".join(med)\n",
    "strr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>           <td>G3</td>        <th>  R-squared:         </th> <td>   0.111</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.070</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   2.737</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Thu, 29 Feb 2024</td> <th>  Prob (F-statistic):</th>  <td>0.00113</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>20:18:56</td>     <th>  Log-Likelihood:    </th> <td> -752.16</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   300</td>      <th>  AIC:               </th> <td>   1532.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   286</td>      <th>  BIC:               </th> <td>   1584.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    13</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "       <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>  <td>   13.2983</td> <td>    2.975</td> <td>    4.470</td> <td> 0.000</td> <td>    7.442</td> <td>   19.154</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>age</th>        <td>   -0.1319</td> <td>    0.167</td> <td>   -0.791</td> <td> 0.429</td> <td>   -0.460</td> <td>    0.196</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Medu</th>       <td>    0.5820</td> <td>    0.212</td> <td>    2.749</td> <td> 0.006</td> <td>    0.165</td> <td>    0.999</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Fedu</th>       <td>    0.0417</td> <td>    0.206</td> <td>    0.202</td> <td> 0.840</td> <td>   -0.365</td> <td>    0.448</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>traveltime</th> <td>   -0.1115</td> <td>    0.311</td> <td>   -0.358</td> <td> 0.721</td> <td>   -0.724</td> <td>    0.501</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>studytime</th>  <td>    0.2029</td> <td>    0.231</td> <td>    0.879</td> <td> 0.380</td> <td>   -0.252</td> <td>    0.657</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>failures</th>   <td>   -0.4329</td> <td>    0.624</td> <td>   -0.694</td> <td> 0.488</td> <td>   -1.660</td> <td>    0.795</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>famrel</th>     <td>    0.1045</td> <td>    0.234</td> <td>    0.447</td> <td> 0.655</td> <td>   -0.356</td> <td>    0.565</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>freetime</th>   <td>    0.1939</td> <td>    0.187</td> <td>    1.038</td> <td> 0.300</td> <td>   -0.174</td> <td>    0.562</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>goout</th>      <td>   -0.4135</td> <td>    0.191</td> <td>   -2.162</td> <td> 0.031</td> <td>   -0.790</td> <td>   -0.037</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Dalc</th>       <td>   -0.0910</td> <td>    0.379</td> <td>   -0.240</td> <td> 0.810</td> <td>   -0.837</td> <td>    0.655</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Walc</th>       <td>   -0.0104</td> <td>    0.204</td> <td>   -0.051</td> <td> 0.959</td> <td>   -0.411</td> <td>    0.390</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>health</th>     <td>   -0.1646</td> <td>    0.130</td> <td>   -1.270</td> <td> 0.205</td> <td>   -0.420</td> <td>    0.091</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>absences</th>   <td>   -0.0693</td> <td>    0.035</td> <td>   -2.003</td> <td> 0.046</td> <td>   -0.137</td> <td>   -0.001</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 2.303</td> <th>  Durbin-Watson:     </th> <td>   2.200</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.316</td> <th>  Jarque-Bera (JB):  </th> <td>   2.086</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.114</td> <th>  Prob(JB):          </th> <td>   0.352</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 2.660</td> <th>  Cond. No.          </th> <td>    332.</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}    &        G3        & \\textbf{  R-squared:         } &     0.111   \\\\\n",
       "\\textbf{Model:}            &       OLS        & \\textbf{  Adj. R-squared:    } &     0.070   \\\\\n",
       "\\textbf{Method:}           &  Least Squares   & \\textbf{  F-statistic:       } &     2.737   \\\\\n",
       "\\textbf{Date:}             & Thu, 29 Feb 2024 & \\textbf{  Prob (F-statistic):} &  0.00113    \\\\\n",
       "\\textbf{Time:}             &     20:18:56     & \\textbf{  Log-Likelihood:    } &   -752.16   \\\\\n",
       "\\textbf{No. Observations:} &         300      & \\textbf{  AIC:               } &     1532.   \\\\\n",
       "\\textbf{Df Residuals:}     &         286      & \\textbf{  BIC:               } &     1584.   \\\\\n",
       "\\textbf{Df Model:}         &          13      & \\textbf{                     } &             \\\\\n",
       "\\textbf{Covariance Type:}  &    nonrobust     & \\textbf{                     } &             \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                    & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{Intercept}  &      13.2983  &        2.975     &     4.470  &         0.000        &        7.442    &       19.154     \\\\\n",
       "\\textbf{age}        &      -0.1319  &        0.167     &    -0.791  &         0.429        &       -0.460    &        0.196     \\\\\n",
       "\\textbf{Medu}       &       0.5820  &        0.212     &     2.749  &         0.006        &        0.165    &        0.999     \\\\\n",
       "\\textbf{Fedu}       &       0.0417  &        0.206     &     0.202  &         0.840        &       -0.365    &        0.448     \\\\\n",
       "\\textbf{traveltime} &      -0.1115  &        0.311     &    -0.358  &         0.721        &       -0.724    &        0.501     \\\\\n",
       "\\textbf{studytime}  &       0.2029  &        0.231     &     0.879  &         0.380        &       -0.252    &        0.657     \\\\\n",
       "\\textbf{failures}   &      -0.4329  &        0.624     &    -0.694  &         0.488        &       -1.660    &        0.795     \\\\\n",
       "\\textbf{famrel}     &       0.1045  &        0.234     &     0.447  &         0.655        &       -0.356    &        0.565     \\\\\n",
       "\\textbf{freetime}   &       0.1939  &        0.187     &     1.038  &         0.300        &       -0.174    &        0.562     \\\\\n",
       "\\textbf{goout}      &      -0.4135  &        0.191     &    -2.162  &         0.031        &       -0.790    &       -0.037     \\\\\n",
       "\\textbf{Dalc}       &      -0.0910  &        0.379     &    -0.240  &         0.810        &       -0.837    &        0.655     \\\\\n",
       "\\textbf{Walc}       &      -0.0104  &        0.204     &    -0.051  &         0.959        &       -0.411    &        0.390     \\\\\n",
       "\\textbf{health}     &      -0.1646  &        0.130     &    -1.270  &         0.205        &       -0.420    &        0.091     \\\\\n",
       "\\textbf{absences}   &      -0.0693  &        0.035     &    -2.003  &         0.046        &       -0.137    &       -0.001     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       &  2.303 & \\textbf{  Durbin-Watson:     } &    2.200  \\\\\n",
       "\\textbf{Prob(Omnibus):} &  0.316 & \\textbf{  Jarque-Bera (JB):  } &    2.086  \\\\\n",
       "\\textbf{Skew:}          &  0.114 & \\textbf{  Prob(JB):          } &    0.352  \\\\\n",
       "\\textbf{Kurtosis:}      &  2.660 & \\textbf{  Cond. No.          } &     332.  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                     G3   R-squared:                       0.111\n",
       "Model:                            OLS   Adj. R-squared:                  0.070\n",
       "Method:                 Least Squares   F-statistic:                     2.737\n",
       "Date:                Thu, 29 Feb 2024   Prob (F-statistic):            0.00113\n",
       "Time:                        20:18:56   Log-Likelihood:                -752.16\n",
       "No. Observations:                 300   AIC:                             1532.\n",
       "Df Residuals:                     286   BIC:                             1584.\n",
       "Df Model:                          13                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "Intercept     13.2983      2.975      4.470      0.000       7.442      19.154\n",
       "age           -0.1319      0.167     -0.791      0.429      -0.460       0.196\n",
       "Medu           0.5820      0.212      2.749      0.006       0.165       0.999\n",
       "Fedu           0.0417      0.206      0.202      0.840      -0.365       0.448\n",
       "traveltime    -0.1115      0.311     -0.358      0.721      -0.724       0.501\n",
       "studytime      0.2029      0.231      0.879      0.380      -0.252       0.657\n",
       "failures      -0.4329      0.624     -0.694      0.488      -1.660       0.795\n",
       "famrel         0.1045      0.234      0.447      0.655      -0.356       0.565\n",
       "freetime       0.1939      0.187      1.038      0.300      -0.174       0.562\n",
       "goout         -0.4135      0.191     -2.162      0.031      -0.790      -0.037\n",
       "Dalc          -0.0910      0.379     -0.240      0.810      -0.837       0.655\n",
       "Walc          -0.0104      0.204     -0.051      0.959      -0.411       0.390\n",
       "health        -0.1646      0.130     -1.270      0.205      -0.420       0.091\n",
       "absences      -0.0693      0.035     -2.003      0.046      -0.137      -0.001\n",
       "==============================================================================\n",
       "Omnibus:                        2.303   Durbin-Watson:                   2.200\n",
       "Prob(Omnibus):                  0.316   Jarque-Bera (JB):                2.086\n",
       "Skew:                           0.114   Prob(JB):                        0.352\n",
       "Kurtosis:                       2.660   Cond. No.                         332.\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ols 분석 시행하여 유의미한 변수를 찾음\n",
    "import statsmodels.formula.api as smf \n",
    "formula = f'G3 ~ {strr}'\n",
    "result = smf.ols(formula, df).fit()\n",
    "result.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "freetime      0.025340\n",
       "health        0.058288\n",
       "traveltime    0.065291\n",
       "famrel        0.066035\n",
       "Dalc          0.076599\n",
       "studytime     0.079444\n",
       "Walc          0.120006\n",
       "Fedu          0.136787\n",
       "age           0.136894\n",
       "goout         0.137101\n",
       "failures      0.155888\n",
       "absences      0.161322\n",
       "Medu          0.212431\n",
       "G1            0.888753\n",
       "G2            0.968069\n",
       "G3            1.000000\n",
       "Name: G3, dtype: float64"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.abs(df.corr()[\"G3\"]).sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "feature=df[[\"Medu\",\"absences\",\"failures\",\"goout\"]]\n",
    "target=df[\"G3\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(feature, \n",
    "                                                    target, \n",
    "                                                    test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\EXAM_ML\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:2667: UserWarning: n_quantiles (100) is greater than the total number of samples (60). n_quantiles is set to n_samples.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler     \n",
    "from sklearn.preprocessing import MinMaxScaler     \n",
    "from sklearn.preprocessing import RobustScaler     \n",
    "from sklearn.preprocessing import SplineTransformer     \n",
    "from sklearn.preprocessing import QuantileTransformer   \n",
    "\n",
    "std_s=StandardScaler()\n",
    "x_train_std=std_s.fit_transform(x_train)\n",
    "std_s=StandardScaler()\n",
    "x_test_std=std_s.fit_transform(x_test)\n",
    "\n",
    "mm_s=MinMaxScaler()\n",
    "x_train_mm=mm_s.fit_transform(x_train)\n",
    "mm_s=MinMaxScaler()\n",
    "x_test_mm=mm_s.fit_transform(x_test)\n",
    "\n",
    "ro_s=RobustScaler()\n",
    "x_train_ro=ro_s.fit_transform(x_train)\n",
    "ro_s=RobustScaler()\n",
    "x_test_ro=ro_s.fit_transform(x_test)\n",
    "\n",
    "sq_s=SplineTransformer()\n",
    "x_train_sq=sq_s.fit_transform(x_train)\n",
    "sq_s=SplineTransformer()\n",
    "x_test_sq=sq_s.fit_transform(x_test)\n",
    "\n",
    "qt_s=QuantileTransformer(n_quantiles=100)\n",
    "x_train_qt=qt_s.fit_transform(x_train)\n",
    "qt_s=QuantileTransformer(n_quantiles=100)\n",
    "x_test_qt=qt_s.fit_transform(x_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data=[x_train_std,x_test_std,x_train_mm,x_test_mm,x_train_ro,x_test_ro,x_train_sq,x_test_sq,x_train_qt,x_test_qt]\n",
    "len(x_data) # 2X5=10개의 X 데이터 셋 마련 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier         \n",
    "from sklearn.ensemble import RandomForestRegressor     \n",
    "from sklearn.svm import SVR     \n",
    "from sklearn.linear_model import LogisticRegression     \n",
    "from sklearn.neighbors import KNeighborsRegressor     \n",
    "from sklearn.linear_model import LinearRegression        \n",
    "from sklearn.multiclass import OneVsRestClassifier     \n",
    "from sklearn.multiclass import OneVsOneClassifier        \n",
    "from sklearn.metrics import mean_squared_error     \n",
    "from sklearn.metrics import mean_absolute_error     \n",
    "from sklearn.metrics import r2_score       \n",
    "from sklearn.metrics import accuracy_score     \n",
    "from sklearn.metrics import precision_score     \n",
    "from sklearn.metrics import recall_score     \n",
    "from sklearn.metrics import f1_score     \n",
    "from sklearn.metrics import confusion_matrix     \n",
    "from sklearn.metrics import classification_report \n",
    "\n",
    "model_list=[KNeighborsClassifier(), \n",
    "            LogisticRegression(), # 여기까지는 분류 측정 지표 계산 가능\n",
    "            RandomForestRegressor(),\n",
    "            SVR(),\n",
    "            KNeighborsRegressor(),\n",
    "            LinearRegression()] \n",
    "#OvO_list=[OneVsRestClassifier(),OneVsOneClassifier()]\n",
    "\n",
    "score_dict=dict()\n",
    "\n",
    "for i in range(0,10,2) : \n",
    "    for n in range(1,20):\n",
    "        for m in ['uniform','distance']:\n",
    "            for s in ['auto','ball_tree','kd_tree','brute']:\n",
    "                model=KNeighborsClassifier(n_neighbors=n,weights=m,algorithm=s)\n",
    "                model.fit(x_data[i], y_train)\n",
    "                train_score=model.score(x_data[i], y_train)                # (1) train_score  \n",
    "                test_score=model.score(x_data[i+1], y_test)                # (2) test_score  \n",
    "                score_dict.setdefault((i,n,m,s),test_score)\n",
    "                \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.21666666666666667"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(score_dict.values())\n",
    "#score_df=pd.DataFrame(score_dict)\n",
    "#score_df#.columns=[\"StandardScaler\",\"MinMaxScaler\",\"RobustScaler\",\"SplineTransformer \",\"QuantileTransformer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 21.6%? 흐하하하....정신이 나갈 것 같다\n",
    "# 이제 로지스틱 회귀와 OvOmodel을 사용해서 해보자 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l1 lbfgs auto\n",
      "l1 lbfgs ovr\n",
      "l1 lbfgs multinomial\n",
      "l1 liblinear auto\n",
      "l1 liblinear ovr\n",
      "l1 liblinear multinomial\n",
      "l1 newton-cg auto\n",
      "l1 newton-cg ovr\n",
      "l1 newton-cg multinomial\n",
      "l1 newton-cholesky auto\n",
      "l1 newton-cholesky ovr\n",
      "l1 newton-cholesky multinomial\n",
      "l1 sag auto\n",
      "l1 sag ovr\n",
      "l1 sag multinomial\n",
      "l1 saga auto\n",
      "l1 saga ovr\n",
      "l1 saga multinomial\n",
      "l2 lbfgs auto\n",
      "l2 lbfgs ovr\n",
      "l2 lbfgs multinomial\n",
      "l2 liblinear auto\n",
      "l2 liblinear ovr\n",
      "l2 liblinear multinomial\n",
      "l2 newton-cg auto\n",
      "l2 newton-cg ovr\n",
      "l2 newton-cg multinomial\n",
      "l2 newton-cholesky auto\n",
      "l2 newton-cholesky ovr\n",
      "l2 newton-cholesky multinomial\n",
      "l2 sag auto\n",
      "l2 sag ovr\n",
      "l2 sag multinomial\n",
      "l2 saga auto\n",
      "l2 saga ovr\n",
      "l2 saga multinomial\n",
      "elasticnet lbfgs auto\n",
      "elasticnet lbfgs ovr\n",
      "elasticnet lbfgs multinomial\n",
      "elasticnet liblinear auto\n",
      "elasticnet liblinear ovr\n",
      "elasticnet liblinear multinomial\n",
      "elasticnet newton-cg auto\n",
      "elasticnet newton-cg ovr\n",
      "elasticnet newton-cg multinomial\n",
      "elasticnet newton-cholesky auto\n",
      "elasticnet newton-cholesky ovr\n",
      "elasticnet newton-cholesky multinomial\n",
      "elasticnet sag auto\n",
      "elasticnet sag ovr\n",
      "elasticnet sag multinomial\n",
      "elasticnet saga auto\n",
      "elasticnet saga ovr\n",
      "elasticnet saga multinomial\n",
      "None lbfgs auto\n",
      "None lbfgs ovr\n",
      "None lbfgs multinomial\n",
      "None liblinear auto\n",
      "None liblinear ovr\n",
      "None liblinear multinomial\n",
      "None newton-cg auto\n",
      "None newton-cg ovr\n",
      "None newton-cg multinomial\n",
      "None newton-cholesky auto\n",
      "None newton-cholesky ovr\n",
      "None newton-cholesky multinomial\n",
      "None sag auto\n",
      "None sag ovr\n",
      "None sag multinomial\n",
      "None saga auto\n",
      "None saga ovr\n",
      "None saga multinomial\n"
     ]
    }
   ],
   "source": [
    "score_dict=dict()\n",
    "for n in [\"l1\",\"l2\",\"elasticnet\",None]:\n",
    "    for m in ['lbfgs','liblinear','newton-cg','newton-cholesky','sag','saga']:\n",
    "        for s in ['auto','ovr','multinomial']:\n",
    "            print(n,m,s)\n",
    "            try :\n",
    "                model=LogisticRegression(max_iter=10000, solver=m, multi_class=s,penalty=n)\n",
    "                model.fit(x_data[0], y_train)\n",
    "                train_score=model.score(x_data[0], y_train)  # (1) train_score  \n",
    "                test_score=model.score(x_data[0+1], y_test)  # (2) test_score  \n",
    "                score_dict.setdefault((0,n,m,s),test_score)\n",
    "            except ValueError :\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최대값: 0.21666666666666667\n",
      "최대값을 가지는 키: [(0, None, 'lbfgs', 'auto'), (0, None, 'lbfgs', 'multinomial'), (0, None, 'newton-cg', 'auto'), (0, None, 'newton-cg', 'multinomial'), (0, None, 'sag', 'auto'), (0, None, 'sag', 'multinomial'), (0, None, 'saga', 'auto'), (0, None, 'saga', 'multinomial')]\n"
     ]
    }
   ],
   "source": [
    "max_value = max(score_dict.values())\n",
    "max_keys = [key for key, value in score_dict.items() if value == max_value]\n",
    "\n",
    "print(\"최대값:\", max_value)\n",
    "print(\"최대값을 가지는 키:\", max_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score : 0.22916666666666666\n",
      " test score : 0.13333333333333333\n"
     ]
    }
   ],
   "source": [
    "model=LogisticRegression(max_iter=10000, solver='lbfgs', multi_class='auto',penalty=None)\n",
    "\n",
    "ovoModel=OneVsOneClassifier(model)\n",
    "ovoModel.fit(x_train, y_train)\n",
    "print(f\"train score : {ovoModel.score(x_train, y_train)}\\n test score : {ovoModel.score(x_test, y_test)}\") # 22.9%!!!! 1% 올렸다 대박.... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([18], dtype=int64)"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 그만해야겠다....역시 이런 데이터로는 공부를 잘하는 학생을 예측할 수 없다\n",
    "# 그 사람만의 성격과 쌓아온 노력을 수치화할 수 없으며, 같이 어울리는 친구들이나\n",
    "# 어떤 해프닝에 의해서도 좌지우지 될 수 있는게 성적이다\n",
    "# 단순히 어떤 조건이 이렇게 해서 공부를 잘 할 수 있다고 판단한 내가 오만했다!\n",
    "# 참고로 나는 \n",
    "feature=df[[\"Medu\",\"absences\",\"failures\",\"goout\"]]\n",
    "\n",
    "# Medu : 어머니의 교육 : 4(고등교육)\n",
    "# absences : 결석 횟수 : 5?\n",
    "# failures : 낙제 횟수 : 0\n",
    "# goout : 외출 : 2(적음)\n",
    "model=LogisticRegression(max_iter=10000, solver='lbfgs', multi_class='auto',penalty=None)\n",
    "model.fit(x_train_std, y_train)\n",
    "model.predict([[4,5,0,2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    300.000000\n",
       "mean      11.790000\n",
       "std        3.153683\n",
       "min        5.000000\n",
       "25%       10.000000\n",
       "50%       11.000000\n",
       "75%       14.000000\n",
       "max       19.000000\n",
       "Name: G3, dtype: float64"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 아니...18점이라고?\n",
    "df[\"G3\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#이 녀석...성적의 범위가 5에서 19인데\n",
    "# 내 점수를 18점이라고 해주다니...감동이다...\n",
    "# 이 모델은 잘 만든 것 같다!(결론)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EXAM_ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
